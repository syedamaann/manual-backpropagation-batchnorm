# Manual Backpropagation & Batch Normalization: Hands-On Exercises
Exercises for manual implementation of backpropagation in a two-layer MLP (Multi-Layer Perceptron) with Batch Normalization, focusing on gradient computation without PyTorch's autograd. It covers dataset creation, parameter initialization, forward and backward passes, and neural network training.

These exercises are inspired by the work of Andrej Karpathy and aim to deepen understanding of backpropagation in neural networks, particularly focusing on efficient gradient computation across layers and Batch Normalization. Through these exercises, participants will gain insights into troubleshooting and optimizing models by understanding backward gradient flow. This skill can be applied to enhance machine learning models.

Join in practicing these exercises to strengthen your AI and machine learning skills with hands-on backpropagation experience.
